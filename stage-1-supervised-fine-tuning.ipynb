{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0d46c084-5adb-4509-aa43-4294524d0677",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers trl peft bitsandbytes datasets\n",
        "!pip install -q rouge_score bert_score\n",
        "!pip install -q evaluate nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "886ab934-7880-481e-a7f6-4bc80ba52bde",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83a829d3-9a32-41d4-af69-4cbc122dd7cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant.*\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "28dddda2-ba01-46fc-bba9-eb81ade6f32c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Config:\n",
        "    base_model_id = \"Qwen/Qwen2.5-3B\"\n",
        "    sft_model_id = \"./qwen-2.5-3b-sft-truthfulqa/sft\"\n",
        "    dpo_model_id = \"./qwen-2.5-3b-dpo-truthfulqa/dpo\"\n",
        "\n",
        "    dataset_id = \"truthfulqa/truthful_qa\"\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5e016fc7-c25e-49ed-b03a-5378afd8df7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_sample(sample):\n",
        "    question = sample.get(\"question\", \"\").strip()\n",
        "    answer = sample.get(\"best_answer\", \"\").strip()\n",
        "\n",
        "    formatted_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "    \n",
        "    return {\"text\": formatted_text}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "32aca67c-596d-4039-8206-b15cac0145ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "full_dataset = load_dataset(config.dataset_id, \"generation\")[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "43f9562f-06e1-4ba5-aa59-99ad1da4da8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_id, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fd309935-2e23-4091-9301-4c163018a55b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_dataset(sample):\n",
        "    formatted = format_sample(sample)\n",
        "    full_text = formatted[\"text\"]\n",
        "\n",
        "    outputs = tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    input_ids = outputs[\"input_ids\"]\n",
        "    attention_mask = outputs[\"attention_mask\"]\n",
        "    offsets = outputs[\"offset_mapping\"]\n",
        "\n",
        "\n",
        "    answer_start_char = full_text.find(\"Answer:\")\n",
        "    if answer_start_char == -1:\n",
        "        labels = input_ids.copy()\n",
        "    else:\n",
        "        labels = []\n",
        "        for idx, (token_id, offset) in enumerate(zip(input_ids, offsets)):\n",
        "            char_start, _ = offset\n",
        "            if char_start >= answer_start_char:\n",
        "                labels.append(token_id)\n",
        "            else:\n",
        "                labels.append(-100)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "62bfb894-3dce-47e3-81b3-928588b8c2af",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b55684b13fc8462eb5434be68956cfb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = full_dataset.map(\n",
        "    tokenize_dataset,\n",
        "    batched=False,\n",
        "    remove_columns=full_dataset.column_names\n",
        ")\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "temp_split = split_dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "\n",
        "dataset_split = {\n",
        "    \"train\": split_dataset[\"train\"],\n",
        "    \"validation\": temp_split[\"train\"],\n",
        "    \"test\": temp_split[\"test\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3fff635e-d25d-4e67-8615-5c934cb5e168",
      "metadata": {},
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "575830a4-7cd2-4582-9eea-54d89dd960b9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb4b11c811e440ae835a31bb22ecd2a7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3f519b1d-6b28-40a0-bf74-16f5fb5229ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "sft_training_args = SFTConfig(\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=100,\n",
        "    num_train_epochs=50,\n",
        "    learning_rate=4e-5,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    seed=42,\n",
        "    output_dir=config.sft_model_id,\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"best\",\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    load_best_model_at_end=True,\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=4,\n",
        "    packing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bbc988d8-2635-4575-a427-2ab5b75b971c",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "aa64e223-0e66-4669-849c-4c99272bbe45",
      "metadata": {},
      "outputs": [],
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "exact_match_metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    predictions, labels = eval_preds\n",
        "\n",
        "    # Handle tuple for predictions\n",
        "    predictions = predictions[0] if isinstance(predictions, tuple) else predictions\n",
        "\n",
        "    # Convert logits to predicted token IDs if needed\n",
        "    if predictions.ndim == 3:\n",
        "        predictions = predictions.argmax(-1)\n",
        "\n",
        "    # Convert tensors to lists\n",
        "    if hasattr(predictions, \"tolist\"):\n",
        "        predictions = predictions.tolist()\n",
        "    if hasattr(labels, \"tolist\"):\n",
        "        labels = labels.tolist()\n",
        "\n",
        "    # Replace -100 in labels with tokenizer.pad_token_id for decoding\n",
        "    labels = [\n",
        "        [token if token != -100 else tokenizer.pad_token_id for token in label_seq]\n",
        "        for label_seq in labels\n",
        "    ]\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Strip extra whitespace\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    bleu_result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    rouge_result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    bertscore_result = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu_result[\"bleu\"],\n",
        "        \"rouge1\": rouge_result[\"rouge1\"],\n",
        "        \"rougeL\": rouge_result[\"rougeL\"],\n",
        "        \"bertscore_f1\": np.mean(bertscore_result[\"f1\"]),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6cd61670-ec5d-4817-b808-92b5b03a0af9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da1871b3850e4079a45b5ccc4af8da5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset (num_proc=4):   0%|          | 0/735 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0400893706854e4b98da58543e5df900",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset (num_proc=4):   0%|          | 0/41 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_training_args,\n",
        "    train_dataset=dataset_split[\"train\"],\n",
        "    eval_dataset=dataset_split[\"validation\"],\n",
        "    peft_config=lora_config,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "39fdf6bc-a8d6-4d48-9f34-cfc86a79e42c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='324' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [324/600 05:41 < 04:52, 0.94 it/s, Epoch 27/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Bertscore F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.441400</td>\n",
              "      <td>2.385911</td>\n",
              "      <td>0.118612</td>\n",
              "      <td>0.501948</td>\n",
              "      <td>0.454086</td>\n",
              "      <td>0.864638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.395600</td>\n",
              "      <td>2.359903</td>\n",
              "      <td>0.114494</td>\n",
              "      <td>0.504731</td>\n",
              "      <td>0.458330</td>\n",
              "      <td>0.866696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.150700</td>\n",
              "      <td>2.314378</td>\n",
              "      <td>0.117668</td>\n",
              "      <td>0.510264</td>\n",
              "      <td>0.463320</td>\n",
              "      <td>0.867438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.443000</td>\n",
              "      <td>2.245841</td>\n",
              "      <td>0.141375</td>\n",
              "      <td>0.521919</td>\n",
              "      <td>0.471874</td>\n",
              "      <td>0.862413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.269000</td>\n",
              "      <td>2.159682</td>\n",
              "      <td>0.158197</td>\n",
              "      <td>0.526713</td>\n",
              "      <td>0.477122</td>\n",
              "      <td>0.863422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.240900</td>\n",
              "      <td>2.069691</td>\n",
              "      <td>0.168470</td>\n",
              "      <td>0.543209</td>\n",
              "      <td>0.488836</td>\n",
              "      <td>0.867555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.192900</td>\n",
              "      <td>1.978427</td>\n",
              "      <td>0.183986</td>\n",
              "      <td>0.572892</td>\n",
              "      <td>0.521685</td>\n",
              "      <td>0.871015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.068300</td>\n",
              "      <td>1.878253</td>\n",
              "      <td>0.187286</td>\n",
              "      <td>0.570884</td>\n",
              "      <td>0.522619</td>\n",
              "      <td>0.871778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.683000</td>\n",
              "      <td>1.783689</td>\n",
              "      <td>0.193743</td>\n",
              "      <td>0.573189</td>\n",
              "      <td>0.526655</td>\n",
              "      <td>0.873448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.753700</td>\n",
              "      <td>1.730028</td>\n",
              "      <td>0.192218</td>\n",
              "      <td>0.574093</td>\n",
              "      <td>0.526623</td>\n",
              "      <td>0.873321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.648900</td>\n",
              "      <td>1.694200</td>\n",
              "      <td>0.185900</td>\n",
              "      <td>0.567586</td>\n",
              "      <td>0.519669</td>\n",
              "      <td>0.872033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.862000</td>\n",
              "      <td>1.662812</td>\n",
              "      <td>0.187965</td>\n",
              "      <td>0.577644</td>\n",
              "      <td>0.528381</td>\n",
              "      <td>0.872499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.680800</td>\n",
              "      <td>1.640183</td>\n",
              "      <td>0.191730</td>\n",
              "      <td>0.578755</td>\n",
              "      <td>0.531377</td>\n",
              "      <td>0.873582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.607100</td>\n",
              "      <td>1.612954</td>\n",
              "      <td>0.192754</td>\n",
              "      <td>0.586422</td>\n",
              "      <td>0.544467</td>\n",
              "      <td>0.874489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>1.597168</td>\n",
              "      <td>0.198803</td>\n",
              "      <td>0.584006</td>\n",
              "      <td>0.547328</td>\n",
              "      <td>0.874791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.594700</td>\n",
              "      <td>1.580410</td>\n",
              "      <td>0.196552</td>\n",
              "      <td>0.594176</td>\n",
              "      <td>0.553486</td>\n",
              "      <td>0.876149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.433100</td>\n",
              "      <td>1.570395</td>\n",
              "      <td>0.201301</td>\n",
              "      <td>0.586439</td>\n",
              "      <td>0.551905</td>\n",
              "      <td>0.875968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.663400</td>\n",
              "      <td>1.560964</td>\n",
              "      <td>0.198304</td>\n",
              "      <td>0.586236</td>\n",
              "      <td>0.550053</td>\n",
              "      <td>0.875967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.564100</td>\n",
              "      <td>1.551482</td>\n",
              "      <td>0.206413</td>\n",
              "      <td>0.590584</td>\n",
              "      <td>0.557035</td>\n",
              "      <td>0.876412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.726300</td>\n",
              "      <td>1.548168</td>\n",
              "      <td>0.209299</td>\n",
              "      <td>0.590546</td>\n",
              "      <td>0.558020</td>\n",
              "      <td>0.876368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.473900</td>\n",
              "      <td>1.545341</td>\n",
              "      <td>0.211758</td>\n",
              "      <td>0.594989</td>\n",
              "      <td>0.564054</td>\n",
              "      <td>0.877040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.624200</td>\n",
              "      <td>1.543136</td>\n",
              "      <td>0.213127</td>\n",
              "      <td>0.594169</td>\n",
              "      <td>0.564008</td>\n",
              "      <td>0.877627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.390400</td>\n",
              "      <td>1.540063</td>\n",
              "      <td>0.210765</td>\n",
              "      <td>0.594898</td>\n",
              "      <td>0.561817</td>\n",
              "      <td>0.876917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.577900</td>\n",
              "      <td>1.532966</td>\n",
              "      <td>0.209453</td>\n",
              "      <td>0.593889</td>\n",
              "      <td>0.565797</td>\n",
              "      <td>0.878055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.444400</td>\n",
              "      <td>1.535904</td>\n",
              "      <td>0.214910</td>\n",
              "      <td>0.594833</td>\n",
              "      <td>0.564614</td>\n",
              "      <td>0.877475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.429300</td>\n",
              "      <td>1.536422</td>\n",
              "      <td>0.209147</td>\n",
              "      <td>0.589586</td>\n",
              "      <td>0.556592</td>\n",
              "      <td>0.877337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.379700</td>\n",
              "      <td>1.534312</td>\n",
              "      <td>0.214634</td>\n",
              "      <td>0.591962</td>\n",
              "      <td>0.560606</td>\n",
              "      <td>0.876820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "train_history = sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3db95195-14f6-422a-ac1c-0320884f76be",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=324, training_loss=1.835309867505674, metrics={'train_runtime': 343.2695, 'train_samples_per_second': 107.059, 'train_steps_per_second': 1.748, 'total_flos': 1.6464668845522944e+16, 'train_loss': 1.835309867505674})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3149c80c-9e8f-49b1-8b1c-bc1e7b7b975b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_history = sft_trainer.evaluate(dataset_split[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "82a15eaa-1e00-49d9-a055-ff9f754266de",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.5612815618515015,\n",
              " 'eval_bleu': 0.285689956313672,\n",
              " 'eval_rouge1': 0.6070493544972422,\n",
              " 'eval_rougeL': 0.5751630982614317,\n",
              " 'eval_bertscore_f1': 0.8836286445943321,\n",
              " 'eval_runtime': 1.53,\n",
              " 'eval_samples_per_second': 26.797,\n",
              " 'eval_steps_per_second': 3.921}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5c77e7f6-f2ff-4249-85e6-41a1ecbe354a",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/teamspace/studios/this_studio'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "567a5629-40cd-4158-acc9-99e72e9238c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_all_files(output_filename='stage-1.zip', directory='qwen-2.5-3b-sft-truthfulqa'):\n",
        "    with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for foldername, subfolders, filenames in os.walk(directory):\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(foldername, filename)\n",
        "                # Skip hidden files and system files if desired\n",
        "                if not filename.startswith('.') and '__pycache__' not in file_path:\n",
        "                    zipf.write(file_path, os.path.relpath(file_path, directory))\n",
        "\n",
        "zip_all_files()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a2a2ff-7e71-4112-9d8f-9e0f0ff292c6",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}